# Materialize

Naiad (Timely Dataflow), Incremental Dataflow, and Materialize are a family
of streaming database systems that combine low-latency stream processing with
iterative and incremental computation.

## Naiad: A Timely Dataflow System

Naiad is a distributed analytics database that can best be thought of an
intersection between Kafka and Hadoop. It takes in streams of data and
materializes incremental views on the data. The internal data forms a cyclic,
directed graph.

### Features

It provides the following features:

1. Structured loops (for low latency)
2. Stateful data processors that can process data without global locks (for
   low latency)
3. Notifications when processing is complete (for consistency)

### Pitch

What Naiad does is nothing new: similar systems can be constructed by
stitching features together. The main innovation is that it does this
_efficiently_: it's a single system that covers analytics, replacing
Kafka/Hadoop/batch processing clusters.


### Timely Dataflow

Nodes in timely dataflow make use of a pull-based event model. Kind of like Rust's channels.

#### Vertexes (nodes in the graph)

Internally Naiad uses a directed cyclic graph of event emitter nodes
(vertexes) that send messages to each other (edges). There are several kinds
of nodes:

1. input: receives a sequence of messages from an external producer
2. output: emits a sequence of messages back to an external consumer

Input and output vertexes will send a message when no more events for a given
epoch will be sent. Or will send a "close" event when no more messages of
_any_ epoch will be sent. Input vertexes are driven by external sources that
provide the epoch, and "close" event, so it's mostly a way to propagate this
through the system.

#### Epochs

Epoch labels are integers and are generated by external producers and sent to
the input nodes.

#### Timestamps and Cycles

Vertexes are organized into _loop contexts_. There are 3 kinds of nodes that
makes up a loop context:

- __ingress node:__ messages going into the context are received here
- __egress node:__ messages leaving the context are sent from here
- __feedback node:__ messages in the context are processed here

Contexts can be nested, but each context must have a __feedback node_ so that
it does work. No empty contexts!

Each message has a timestamp that keeps track of the epoch, and the loop
contexts. That way it can keep track of the current generation, and how it's
progressed throughout the system.

```txt
epoch-[(counter for loop context), ...more counters]
```

Timestamps work like a stack: ingress nodes push a zero to the array.
Feedback nodes increment the counter by 1. Egress nodes pop the latest value
from the array. Put together with the other rules it ensures we always know
how far we are in our graph processing.

- ingress: pushed a `0` to the counter array
- egress: pops the latest value from the counter array
- feedback: increments the latest value of the counter array by 1


### Messaging

Each vertex (node) implements a way to receive messages. Each message has an
event name, data payload, and timestamp. Sending messages is done by calling
a system-wide emitter that sends the message to all listeners registered.

```rust
/// A local node (vertex) that can receive messages.
trait Node: Sized {
    /// Receive an event.
    fn on_recv(&mut self, edge: String, message: String, timestamp: Timestamp);

    /// Receive a notification that no more events from before or at the
    /// timestamp will be emitted.
    fn on_notify(&mut self, timestamp: Timestamp);

    /// Send an event.
    fn send_by(&self, edge: String, message: String, timestamp: Timestamp) {
        // implementation provided
    }

    /// Notify that no more events from the current epoch will be emitted.
    fn notify_at(&self, timestamp: Timestamp) {
        // implementation provided; it also sends the name of the Node.
    }
}
```

The following rules apply:

- Calls to `send_by` map to calls of `on_recv`
- Calls to `notify_at` map to calls of `on_notify`
- Events are queued in the system
- After a timestamp T is sent through `notify_with`, future timestamps for
  `send_by` and `notify_at` must always be later than T. This ensures that we
  can mark the epoch as "done", and work can be processed without needing to
  worry more work may need to be processed again later.


### Point Stamps

Calling `notify_at` by itself is not specific enough to identify an event. We
need to provide more context, so each call includes the name of the Node as
well. For calls to `send_by` we already have an id: the event name + timestamp.

- __send_by:__ the "pointstamp" ID is `(timestamp, edge name)` or as we call
  it `(timestamp, event name)`
- __notify_at:__ the "pointstamp" ID is `(timestamp, vertex name)` -- or
  `(timestamp, node name)`

Or in short:

```txt
type PointStamp = (TimeStamp, Edge | Vertex);
                              |-----------|
                                 Location
```
